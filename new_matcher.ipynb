{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test dataset\n",
    "def concatImages(pt1,pt2):\n",
    "  e1 = cv2.imread(pt1,cv2.IMREAD_GRAYSCALE)\n",
    "  e2 = cv2.imread(pt2,cv2.IMREAD_GRAYSCALE)\n",
    "  return np.concatenate((e1,e2),axis=1)\n",
    "\n",
    "# scale pixels\n",
    "def prep_pixels(tensor_arr):\n",
    "  # convert from integers to floats\n",
    "  train_norm = tensor_arr.astype('float32')\n",
    "  # normalize to range 0-1\n",
    "  train_norm = train_norm / 255.0\n",
    "  # return normalized images\n",
    "  return train_norm\n",
    "\n",
    "def loadIntraClass(db_root):\n",
    "  trainX = []\n",
    "  trainY = []\n",
    "  dirs = os.listdir(db_root)\n",
    "  for dir in dirs:\n",
    "    files = os.listdir(db_root+dir)\n",
    "    for k1 in range(len(files)-1):\n",
    "      for k2 in range(k1+1, len(files)):\n",
    "        trainX.append(concatImages(f'{db_root}{dir}/{files[k1]}', f'{db_root}{dir}/{files[k2]}'))\n",
    "        trainY.append([1,0])\n",
    "  return trainX, trainY\n",
    "\n",
    "def loadInterClass(db_root):\n",
    "  random.seed(100)\n",
    "  trainX = []\n",
    "  trainY = []\n",
    "  dirs = os.listdir(db_root)\n",
    "  for dir1 in range(len(dirs)-1):\n",
    "    files1 = os.listdir(db_root+dirs[dir1])\n",
    "    for dir2 in range(dir1, len(dirs)):\n",
    "      files2 = os.listdir(db_root+dirs[dir2])\n",
    "      n1 = len(files1)\n",
    "      n2 = len(files2)\n",
    "      if(n1*n2>77):\n",
    "        #generate 77 non matches\n",
    "        samples = []\n",
    "        while(len(samples)<77):\n",
    "          k1 = random.randint(0,n1-1)\n",
    "          k2 = random.randint(0,n2-1)\n",
    "          if((k1,k2) not in samples):\n",
    "            samples.append((k1,k2))\n",
    "        for (k1,k2) in samples:\n",
    "          trainX.append(concatImages(f'{db_root}{dirs[dir1]}/{files1[k1]}', f'{db_root}{dirs[dir2]}/{files2[k2]}'))\n",
    "          trainY.append([0,1])\n",
    "      else:\n",
    "        for k1 in files1:\n",
    "          for k2 in files2:\n",
    "            trainX.append(concatImages(f'{db_root}{dirs[dir1]}/{k1}', f'{db_root}{dirs[dir2]}/{k2}'))\n",
    "            trainY.append([0,1])\n",
    "  return trainX, trainY\n",
    "\n",
    "def load_tensors(db_root):\n",
    "  X,Y = loadIntraClass(db_root)\n",
    "  X1,Y1 = loadInterClass(db_root)\n",
    "  X.extend(X1)\n",
    "  Y.extend(Y1)\n",
    "  X = tf.convert_to_tensor(X)\n",
    "  X = prep_pixels(X)\n",
    "  Y = tf.convert_to_tensor(Y)\n",
    "  X.reshape((X.shape[0], 160, 280, 1))\n",
    "  print(X.shape)\n",
    "  return X,Y\n",
    "\n",
    "def load_dataset(db_root):\n",
    "  # load dataset\n",
    "  trainX, trainY = load_tensors(f'{db_root}/train/edges/')\n",
    "  # trainX = []\n",
    "  # trainY = []\n",
    "  testX, testY = load_tensors(f'{db_root}/test/edges/')\n",
    "  return trainX, trainY, testX, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12856, 160, 280)\n",
      "(5610, 160, 280)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY, testX, testY = load_dataset('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12856, 2) (5610, 2)\n"
     ]
    }
   ],
   "source": [
    "print(trainY.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model():\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(160, 280, 1)))\n",
    "  model.add(MaxPooling2D((3, 3)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(200, activation='relu', kernel_initializer='he_uniform'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dense(2, activation='softmax'))\n",
    "  # compile model\n",
    "  opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness(trainX, trainY, testX, testY):\n",
    "  # load dataset\n",
    "  \n",
    "  # prepare pixel data\n",
    "  #trainX = prep_pixels(trainX)\n",
    "  \n",
    "  # evaluate model\n",
    "  model = define_model()\n",
    "  trainX, testX, trainY, testY = train_test_split(trainX, trainY, test_size=0.1, random_state=0)\n",
    "  model.fit(trainX, trainY, epochs=5, batch_size=256, validation_data=(testX,testY))\n",
    "  \n",
    "  print(trainX.shape, testX.shape)\n",
    "  # learning curves\n",
    "  #summarize_diagnostics(histories)\n",
    "  # summarize estimated performance\n",
    "  #model.save_weights('./tf_models/matcher2.pth')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "46/46 [==============================] - 57s 1s/step - loss: 0.4334 - accuracy: 0.8257 - val_loss: 0.7794 - val_accuracy: 0.7535\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 60s 1s/step - loss: 0.2363 - accuracy: 0.9150 - val_loss: 0.2867 - val_accuracy: 0.8919\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 61s 1s/step - loss: 0.1862 - accuracy: 0.9332 - val_loss: 0.2374 - val_accuracy: 0.9090\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 64s 1s/step - loss: 0.1561 - accuracy: 0.9443 - val_loss: 0.2197 - val_accuracy: 0.9199\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 68s 1s/step - loss: 0.1362 - accuracy: 0.9499 - val_loss: 0.2239 - val_accuracy: 0.9160\n",
      "(11570, 160, 280) (1286, 160, 280)\n"
     ]
    }
   ],
   "source": [
    "model = run_test_harness(trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x20cc28eec50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model()\n",
    "model.load_weights('./tf_models/matcher3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 8s 85ms/step - loss: 0.4452 - accuracy: 0.8236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4451504945755005, 0.8235714435577393]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testX[:2800], testY[:2800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./tf_models/matcher4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n",
      "[[0.8165002  0.18349978]\n",
      " [0.08199222 0.91800773]\n",
      " [0.15132786 0.84867215]\n",
      " [0.45578888 0.5442111 ]\n",
      " [0.09796178 0.9020382 ]\n",
      " [0.07260024 0.92739975]\n",
      " [0.7187     0.2813    ]\n",
      " [0.6118424  0.38815764]\n",
      " [0.8584885  0.14151153]\n",
      " [0.6211367  0.3788633 ]]\n"
     ]
    }
   ],
   "source": [
    "X = model.predict(testX[-10:])\n",
    "print(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b153a99c3a4485f0e00ad4da8a083284092373fd5e464cfa1a038656a443ffa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
