{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scale pixels\n",
    "def prep_pixels(tensor_arr):\n",
    "  # convert from integers to floats\n",
    "  train_norm = tensor_arr.astype('float32')\n",
    "  # normalize to range 0-1\n",
    "  train_norm = train_norm / 255.0\n",
    "  # return normalized images\n",
    "  return train_norm\n",
    "\n",
    "# load train and test dataset\n",
    "def appendImages(pt1,pt2):\n",
    "  e1 = cv2.imread(pt1,cv2.IMREAD_GRAYSCALE)\n",
    "  e2 = cv2.imread(pt2,cv2.IMREAD_GRAYSCALE)\n",
    "  return np.concatenate((e1,e2),axis=1)\n",
    "\n",
    "def loadData(db_root,filepath, intra=True):\n",
    "  trainX = []\n",
    "  trainY = []\n",
    "  file = open(filepath,'r')\n",
    "  count=0\n",
    "  for line in file:\n",
    "    l = line.split(',')\n",
    "    sad = float(l[-1])\n",
    "    if(sad<7 and intra == True):\n",
    "      trainX.append(appendImages(f'{db_root}{l[0]}', f'{db_root}{l[1]}'))\n",
    "      trainY.append([1,0])\n",
    "      count+=1\n",
    "    if(sad>7 and intra==False):\n",
    "      trainX.append(appendImages(f'{db_root}{l[0]}', f'{db_root}{l[1]}'))\n",
    "      trainY.append([0,1])\n",
    "      count+=1\n",
    "    if(count>=1000):\n",
    "      break\n",
    "  print(intra, count)\n",
    "  file.close()\n",
    "  return trainX, trainY\n",
    "\n",
    "def load_dataset(db_root):\n",
    "  # load dataset\n",
    "  trainX,trainY = loadData(db_root,'IntraClassSad1.txt', True)\n",
    "  print(\"Positives \",len(trainX))\n",
    "  X,Y = loadData(db_root,'InterClassSad1.txt',False)\n",
    "  print(\"Negatives \",len(X))\n",
    "  trainX.extend(X)\n",
    "  trainY.extend(Y)\n",
    "  trainX = tf.convert_to_tensor(trainX)\n",
    "  trainY = tf.convert_to_tensor(trainY)\n",
    "  trainX = prep_pixels(trainX)\n",
    "  trainX, testX, trainY, testY = train_test_split(trainX, trainY, test_size=0.2)\n",
    "  # reshape dataset to have a single channel\n",
    "  trainX = trainX.reshape((trainX.shape[0], 160, 280, 1))\n",
    "  testX = testX.reshape((testX.shape[0], 160, 280, 1))\n",
    "  print('ALL ', trainX.shape)\n",
    "  # one hot encode target values\n",
    "  assert(trainY.shape[1] == testY.shape[1])\n",
    "  return trainX, trainY, testX, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 1000\n",
      "Positives  1000\n",
      "False 1000\n",
      "Negatives  1000\n",
      "ALL  (1600, 160, 280, 1)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY, testX, testY = load_dataset('features/m1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2) (400, 2)\n"
     ]
    }
   ],
   "source": [
    "print(trainY.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(160, 280, 1)))\n",
    "  model.add(MaxPooling2D((3, 3)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(200, activation='relu', kernel_initializer='he_uniform'))\n",
    "  model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "  model.add(Dense(2, activation='softmax'))\n",
    "  # compile model\n",
    "  opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness(trainX, trainY, testX, testY):\n",
    "  # load dataset\n",
    "  \n",
    "  # prepare pixel data\n",
    "  trainX = prep_pixels(trainX)\n",
    "  \n",
    "  # evaluate model\n",
    "  model = define_model()\n",
    "  #trainX, testX, trainY, testY = train_test_split(trainX, trainY, test_size=0.2, random_state=0)\n",
    "  model.fit(trainX, trainY, epochs=5, batch_size=256)\n",
    "  \n",
    "  print(trainX.shape, testX.shape)\n",
    "  # learning curves\n",
    "  #summarize_diagnostics(histories)\n",
    "  # summarize estimated performance\n",
    "  model.save_weights('./tf_models/matcher.pth')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 21s 2s/step - loss: 0.6924 - accuracy: 0.6944\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.6899 - accuracy: 0.5081\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.6863 - accuracy: 0.5081\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.6813 - accuracy: 0.7356\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.6763 - accuracy: 0.8281\n",
      "(1600, 160, 280, 1) (400, 160, 280, 1)\n"
     ]
    }
   ],
   "source": [
    "model = run_test_harness(trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 90ms/step - loss: 0.4625 - accuracy: 0.7700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4624626040458679, 0.7699999809265137]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x199187fd840>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model()\n",
    "model.load_weights('./tf_models/matcher2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestSet(db_root):\n",
    "  random.seed(0)\n",
    "  testX = []\n",
    "  testY = []\n",
    "  dirs = os.listdir(db_root)\n",
    "\n",
    "  #negatives\n",
    "  for d1 in range(len(dirs)-1):\n",
    "    files1 = os.listdir(f'{db_root}/{dirs[d1]}')\n",
    "    for d2 in range(d1+1, len(dirs)):\n",
    "      files2 = os.listdir(f'{db_root}/{dirs[d2]}')\n",
    "      samples = []\n",
    "      for i in range(7):\n",
    "        k1 = random.randint(0,len(files1)-1)\n",
    "        k2 = random.randint(0, len(files2)-1)\n",
    "        samples.append((k1,k2))\n",
    "      for k in samples:\n",
    "        im1 = cv2.imread(f'{db_root}/{dirs[d1]}/{files1[k[0]]}', 0)\n",
    "        im2 = cv2.imread(f'{db_root}/{dirs[d2]}/{files2[k[1]]}', 0)\n",
    "        testX.append(np.concatenate((im1,im2), axis=1))\n",
    "        testY.append([0,1])\n",
    "  \n",
    "  \n",
    "  count=0\n",
    "  #positives\n",
    "  for d in range(len(dirs)):\n",
    "    files = os.listdir(f'{db_root}/{dirs[d]}')\n",
    "    for k1 in range(len(files)-1):\n",
    "      for k2 in range(k1+1,len(files)):\n",
    "        im1 = cv2.imread(f'{db_root}/{dirs[d]}/{files[k1]}', 0)\n",
    "        im2 = cv2.imread(f'{db_root}/{dirs[d]}/{files[k2]}', 0)\n",
    "        testX.append(np.concatenate((im1,im2), axis=1))\n",
    "        testY.append([1,0])\n",
    "        count+=1\n",
    "        if(count==5):\n",
    "            count=0\n",
    "            break\n",
    "  testX1 = tf.convert_to_tensor(testX)\n",
    "  testX1 = testX1.reshape((testX1.shape[0], 160, 280, 1))\n",
    "  testX1 = prep_pixels(testX1)\n",
    "  testY1 = tf.convert_to_tensor(testY)\n",
    "  print(testX1.shape, testY1.shape)\n",
    "  return testX1, testY1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(939, 160, 280, 1) (939, 2)\n"
     ]
    }
   ],
   "source": [
    "testX1,testY1 = createTestSet('features/test/m1/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 3s 90ms/step - loss: 0.6646 - accuracy: 0.7710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6645694375038147, 0.771032989025116]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testX1, testY1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393 546\n"
     ]
    }
   ],
   "source": [
    "c1=0\n",
    "c2=0\n",
    "for k in testY1:\n",
    "    if(k[0] == 0 and k[1] == 1):\n",
    "        c1 +=1\n",
    "    else:\n",
    "        c2+=1\n",
    "print(c1,c2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b153a99c3a4485f0e00ad4da8a083284092373fd5e464cfa1a038656a443ffa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
